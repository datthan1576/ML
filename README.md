Lesson 1: Introduction to Machine Learning

- Using Sklearn for Iris dataset.
- Binary classification, multiclass classification.

Lesson 2: Linear classifier and stochastic gradient

- Stochastic gradient in practice.
- Maximum Likelihood and Regularization L1,L2.
- Find optimize regularization using LOO.

Lesson 3: Neural Networks: Gradient Optimization Techniques

- Autograd.
- MLP for MNIST.
- Tuning hyperparameters for MLP.

Lesson 4: Metric classification and regression methods

- kNN, kernel-kNN, Parzen window method, potential function method.
- Reference element selection, STOLP, Nadarai Watson formula.

Lesson 5: Support vector machine

- SVM , kernel-SVM for classification, regression.
- SVM feature.

Lesson 6: Multidimensional Linear Regression

- Multidimendional Linear Regression, SVD, regularization for MLR using SVD.
- Dependence of the approximation quality on the condition number.
- PCA on MNIST.
- PCA for images.

Lesson 7: Nonlinear Regression

- Non-linear regression example.
- Compare gradient descent, Newton-Raphson and Newton-Gauss.
- Generalized linear models: optimal sample size.
- Loss function for the problem of finding close sentences.
- Convergence visualization of the Newton Raphson method and the stochastic gradient.

Lesson 8: Model Selection Criteria and Feature Selection Methods

- Model quality assessment: external and internal criteria.
- Feature selection: exhaustive search, Add algorithm, Add-Del algorithm.
- Precision,Recall.
- Example of information retrieval task.

Lesson 9: Logical classification methods

- Logical classifier implementation.
- Informative criteria.
- Decision list, simple implementation.
- Decision tree.
- Random forest.

Lesson 10: Search for association rules

- Statement of the problem of association rules.
- Synthetic example.
- Example of real data from Kaggle.
- Apriori algorithm.
- FP-growth algorithm.
- Generalization for real data.
- Generalized association rules.

Lesson 11: Linear Ensembles

- DummyEnsemble.
- AdaBoost.
- Gradient boosting, XGBoost.
- An example of real data from kaggle.
- RandomForest.
- Mixture Of Expert.

Lesson 12: Advanced Ensembling Techniques

- ComBoost.
- Gradient Boosting.
- XGBoost.
- CatBoost.

Lesson 13: Bayesian theory of classification

- Maximum Likelihood Principle: Visualization.
- Density reconstruction from empirical data.
- Using LOO to select the window width.
- Naive Bayes classifier.

Lesson 14: Clustering and semi-supervised learning

- Clustering examples.
- K-means.
- DBSCAN.
- Hierarchical clustering.
- Semi-supervised learning.
- Self-training, 1970.
- Unlabeled data in deep learning.

Lesson 15: Deep Neural Networks

Lesson 16: AutoEncoder,GAN

Lesson 17: Tokenization,Word2Vec(Fasttext)

Lesson 18: Attention.Transformer

Lesson 19: Modeling

Lesson 20: Homework

Lesson 21: Learning to rank

Lesson 22: Recommender Systems

Lesson 23: Time Series Analysis

Lesson 24: Online Learning

Lesson 25: Reinforcement Learning

Lesson 26: Active Learning





